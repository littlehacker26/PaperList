# Content

# 基础理论
  - NIPS2019, [Distributional Reinforcement Learning for Energy-Based Sequential Models](https://arxiv.org/pdf/1912.08517.pdf)
  - ICLR workshop 2021,[ON FEATURE DIVERSITY IN ENERGY-BASED MODELS](https://openreview.net/pdf?id=ks3Q08yy66r)
  - Arixiv 2020, [SUPERVISED CONTRASTIVE LEARNING FOR PRETRAINED LANGUAGE MODEL FINE-TUNING](https://arxiv.org/abs/2011.01403),&[note]()
  - Arixiv 2020,[Neural Passage Retrieval with Improved Negative Contrast](https://arxiv.org/pdf/2010.12523.pdf)
  - ICLR2022,[MEMORIZING TRANSFORMERS](https://arxiv.org/pdf/2203.08913.pdf)



# 对话任务
  -  ACL2020,[A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://www.aclweb.org/anthology/2020.acl-main.563/) &[note](./note//2020_10.md)
  - EMNLP2020,[Group-wise Contrastive Learning for Neural Dialogue Generation](https://arxiv.org/abs/2009.07543) &[note](.note//2020_11.md)
  - EMNLP2020,[Multi-turn Response Selection using Dialogue Dependency Relations](https://arxiv.org/abs/2010.01502)
  - EMNLP2020,[Structured Attention for Unsupervised Dialogue Structure Induction](https://www.aclweb.org/anthology/2020.emnlp-main.148/)&[note](./note/2020_11.md)
  - AAAI2019,[A Deep Sequential Model for Discourse Parsing on Multi-Party Dialogues](https://arxiv.org/pdf/1812.00176.pdf)&[note](./note/2020_11.md)
  - AAAI2020,[Who Did They Respond to? Conversation Structure Modeling Using Masked Hierarchical Transformer](https://arxiv.org/abs/1911.10666)&[note]()
  - ACL2020,[Towards Emotion-aided Multi-modal Dialogue Act Classification](https://www.aclweb.org/anthology/2020.acl-main.402/)

## 对话检索
  - SIGIR2020,[Few-Shot Generative Conversational Query Rewriting](https://www.microsoft.com/en-us/research/publication/few-shot-generative-conversational-query-rewriting/) & [note](./note/2020_09.md)
  - WWWW2020, [Leading Conversational Search by Suggesting Useful Questions.](www.baidu.com)

## 对话结构
  - AAAI2018,[Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs](https://arxiv.org/pdf/1709.04005.pdf)&[note]()
  - ACL2016,[Addressee and Response Selection for Multi-Party Conversation](https://www.aclweb.org/anthology/D16-1231/)&[note]()
  - EMNLP 2019,[Who Is Speaking to Whom? Learning to Identify Utterance Addressee in Multi-Party Conversations](https://www.aclweb.org/anthology/D19-1199/)&[note]()
  - Arixiv2020,[Online Conversation Disentanglement with Pointer Networks](https://arxiv.org/pdf/2010.11080.pdf)

## 对话生成
- CIKM2020,[Ranking Enhanced Dialogue Generation](https://arxiv.org/pdf/2008.05640.pdf )
- NAACL2020,[A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation](https://arxiv.org/pdf/2010.11140.pdf)
- Arixiv2020,[Few-shot Natural Language Generation for Task-Oriented Dialogue Generation](https://arxiv.org/pdf/2010.11140.pdf)


#多模态对话情感分类
 - IEEE Trans on Affective Computing,2020. [Adapted Dynamic Memory Network for Emotion Recognition in Conversation](https://ieeexplore.ieee.org/abstract/document/9128015/) &[note](./note/2020_09.md)
 - AAAI2020, [Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network](https://arxiv.org/pdf/1911.09075.pdf)&[note](./note/2020_09.md)
 - Arxiv 2020,[Multi-Task Learning with Auxiliary Speaker Identification for ConversationalEmotion Recognition](https://arxiv.org/abs/2003.01478)&[note]("./note/2020_10.md")
 - EMMLP,2019, [DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation]() [&note](./note/2020_10.md)
 - Arixiv,2019,[PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition](https://arxiv.org/pdf/1910.08916.pdf) &[note](./note/2020_10.md)
 - AAAI,2020,[Sentiment Classification in Customer Service Dialogue with Topic-Aware Multi-Task Learning](https://ojs.aaai.org//index.php/AAAI/article/view/6454)&[note](./note/2020_10.md)
 - arxiv 2020,[Knowledge-Enriched Transformer for Emotion Detection in Textual Conversations](https://arxiv.org/pdf/1909.10681.pdf),&[note](./note/2020_10.md)
 - EMNLP 2020,[COSMIC: COmmonSense knowledge for eMotion Identification in Conversations](https://arxiv.org/abs/2010.02795),&[note](./note/2020_11.md)
 - AAAI 2021,[DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion Recognition](https://arxiv.org/pdf/2012.08695.pdf)
 - Arixiv,2020,[A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation](https://arxiv.org/pdf/2012.14781.pdf)


## 多模态
- AAAI 2019,[Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors](https://www.aaai.org/ojs/index.php/AAAI/article/view/4706) &[note]("./note/2020_10.md")
- ACL 2019,[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://arxiv.org/pdf/1906.00295.pdf) &[note](./note/2020_10.md)
- AAAI,2019,[Found in Translation:Learning Robust Joint Representations by Cyclic Translations Between Modalities](https://arxiv.org/pdf/1812.07809.pdf)&[note](./note/2020_10.md)

# Instruction Tuning
- [Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)
- nips2022, [Training language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)

# Knowledge-enhanced CTG
- CSUR2022,[A Survey of Knowledge-Enhanced Text Generation](https://arxiv.org/pdf/2010.04389.pdf)
- CSUR 2022,[Recent Advances in Retrieval-Augmented Text Generation](https://dl.acm.org/doi/abs/10.1145/3477495.3532682)
- NIPS2022,[Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning](https://arxiv.org/pdf/2205.14704.pdf)
- AAAI2022,[RetGen: A Joint Framework for Retrieval and Grounded Text Generation Modeling](https://ojs.aaai.org/index.php/AAAI/article/view/21429)
- arixiv2023,[REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652)
- AAAI2023,[Rethinking with Retrieval: Faithful Large Language Model Inference](https://arxiv.org/abs/2301.00303)
- Arixiv2023.[Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2302.05578)

# prompt learning
- EMNLP2022, [MemPrompt: Memory-assisted Prompt Editing with User Feedback](https://arxiv.org/pdf/2201.06009.pdf)
- EMNLP2022, [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://preview.aclanthology.org/emnlp-22-ingestion/2022.emnlp-main.174.pdf)
- ICLR2023,[Progressive Prompts: Continual Learning for Language Models](https://arxiv.org/pdf/2301.12314.pdf)
- EMNLP2022,[Vector-Quantized Input-Contextualized Soft Prompts for Natural Language Understanding](https://arxiv.org/abs/2205.11024)
- Arixic,[Instance-aware Prompt Learning for Language Understanding and Generation](https://arxiv.org/abs/2201.07126)
- EMNLP2022,[IDPG: An Instance-Dependent Prompt Generation Method](https://aclanthology.org/2022.naacl-main.403.pdf)
- EMNLP2022,[ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts](https://aclanthology.org/2022.emnlp-main.446/)
- COLONG2022,[Context-Tuning: Learning Contextualized Prompts for Natural Language Generation](https://arxiv.org/pdf/2201.08670.pdf)

# In context-learning
- arixic2022, [Memory Augmented Large Language Models are Computationally Universal](https://arxiv.org/abs/2301.04589)

# 生成综述
-ICML2018,[https://arxiv.org/pdf/1703.00955.pdf](https://arxiv.org/pdf/1703.00955.pdf)

# 语言模型
-ICLR2021,[ANYTIME SAMPLING FOR AUTOREGRESSIVE MODELS VIA ORDERED AUTOENCODING](https://openreview.net/pdf?id=TSRTzJnuEBS)

# 逻辑推理
- Arixic,[Deductive Verification of Chain-of-Thought Reasoning](https://arxiv.org/pdf/2306.03872.pdf)
- arixiv2022, [Let's Verify Step by Step](https://arxiv.org/abs/2305.20050)
- arixiv2023,[Certified Reasoning with Language Models](https://arxiv.org/pdf/2306.04031.pdf)
- [An Empirical Study on Challenging Math Problem Solving with GPT-4](https://arxiv.org/pdf/2306.01337.pdf)


# Cyber Security
- Arixic, [Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?](https://arxiv.org/pdf/2306.01754.pdf)


# CTG LLM
- Arixiv 2023 [Taming AI Bots: Controllability of Neural States in Large Language Models](https://arxiv.org/abs/2305.18449)

# 可控文本生成[2022]
- Nips2022, [COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics](https://openreview.net/pdf?id=TiZYrQ-mPup)
- EMNLP2022,[DisCup: Discriminator Cooperative Unlikelihood Prompt Tuning for Controllable Text Generation](https://arxiv.org/abs/2210.09551)
- ACL2022,[Mix and Match: Learning-free Controllable Text Generation using Energy Language Models](https://arxiv.org/pdf/2203.13299.pdf)
- Data Mining and Knowledge Discovery, [Controlling hallucinations at word level in data-to-text generation](https://link.springer.com/article/10.1007/s10618-021-00801-4#Sec1)
- ACL2022,[Fine-Grained Controllable Text Generation Using Non-Residual Prompting](https://openreview.net/pdf?id=poQNS7GAgBJ)
- Arixiv,[Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation](https://arxiv.org/abs/2204.13362)
- EMNLP2020, [Template Guided Text Generation for Task-Oriented Dialogue](https://aclanthology.org/2020.emnlp-main.527/)
- EMNLP2022,[A Distributional Lens for Multi-Aspect Controllable Text Generation](https://arxiv.org/abs/2210.02889))
- Nips 2022,[Diffusion-LM Improves Controllable Text Generation](https://arxiv.org/abs/2205.14217)
- WSDM2022,[Friendly Conditional Text Generator](https://dl.acm.org/doi/pdf/10.1145/3539597.3570364)



# 可控文本生成
- ICLR2020,[Plug and Play Language Models: A Simple Approach to Controlled Text Generation](https://arxiv.org/abs/1912.02164)
- ICLR2021,[A DISTRIBUTIONAL APPROACH TO CONTROLLED TEXT GENERATION](https://openreview.net/pdf?id=jWkw45-9AbL)
- Arixiv,[ Controllable Generation from Pre-trained Language Models via Inverse Prompting](https://arxiv.org/pdf/2103.10685.pdf)
- arixiv,[Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes](https://arxiv.org/pdf/2104.04039.pdf)
- ICLR,2021, [COCON: A SELF-SUPERVISED APPROACH FOR CONTROLLED TEXT GENERATION](https://arxiv.org/pdf/2006.03535.pdf)
- ICLR,2020.[GEDI: GENERATIVE DISCRIMINATOR GUIDED SEQUENCE GENERATION](https://arxiv.org/pdf/2009.06367.pdf)
- ICLR,2019.[CTRL: A CONDITIONAL TRANSFORMER LANGUAGE MODEL FOR CONTROLLABLE GENERATION](https://arxiv.org/pdf/1909.05858.pdf)
- ICML2020,[Discriminative Adversarial Search for Abstractive Summarization](https://arxiv.org/pdf/2002.10375.pdf)
- EMNLP2020,[If Beam Search is the Answer, What was the Question?](https://arxiv.org/pdf/2010.02650.pdf)
- CCOLING2020,[Exploring Controllable Text Generation Techniques](https://arxiv.org/pdf/2005.01822.pdf)
- Arixiv 2021,[FUDGE: Controlled Text Generation With Future Discriminators](https://arxiv.org/pdf/2104.05218.pdf)
- Arixiv 2020,[DEXPERTS: On-the-Fly Controlled Text Generation with Experts and Anti-Experts](https://arxiv.org/pdf/2105.03023.pdf)
- NIPS2020,[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf) 【强化学习】
- ICML2020,[Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf)【强化学习】
- NIPS2019,[Bias Correction of Learned Generative Models using Likelihood-Free Importance Weighting](https://arxiv.org/abs/1906.09531)
- ICLR2020,[Residual Energy-Based Models for Text Generation](https://arxiv.org/abs/2004.11714)
- TACL2020,[How Can We Know What Language Models Know?](https://arxiv.org/abs/1911.12543)
- EMNLP2020,[POINTER: Constrained Progressive Text Generation via Insertion-based Generative Pre-training](https://arxiv.org/pdf/2005.00558.pdf)
- Arixiv2021,[Long Text Generation by Modeling Sentence-Level and Discourse-Level Coherence](https://arxiv.org/pdf/2105.08963.pdf)
- ACL2020,[Pre-train and Plug-in: Flexible Conditional Text Generation with Variational Auto-Encoders](https://arxiv.org/pdf/1911.03882.pdf)
- AAAI2019,[CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling](https://arxiv.org/pdf/1811.10996.pdf)
- AAAI2020,[Complementary Auxiliary Classifiers for Label-Conditional Text Generation](https://www.microsoft.com/en-us/research/publication/complementary-auxiliary-classifiers-for-label-conditional-text-generation/)
- ACL2021,[Mention Flags (MF): Constraining Transformer-based Text Generators](https://aclanthology.org/2021.acl-long.9.pdf)
- Arixiv2021,[Transformer-based Conditional Variational Autoencoder for Controllable Story Generation](https://arxiv.org/pdf/2101.00828.pdf)
- EMNLP 2020,[PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation](https://arxiv.org/pdf/2010.02301.pdf)
- EMNLP2021,[Parallel Refinements for Lexically Constrained Text Generation with BART](https://arxiv.org/pdf/2109.12487.pdf)
- EMNLP2020,[Gradient-guided Unsupervised Lexically Constrained Text Generation](https://aclanthology.org/2020.emnlp-main.701/)
- NIPS2021,[A Causal Lens for Controllable Text Generation](https://arxiv.org/pdf/2201.09119.pdf)
- NIPS2021,[Controlled Text Generation as Continuous Optimization with Multiple Constraints](https://proceedings.neurips.cc/paper/2021/hash/79ec2a4246feb2126ecf43c4a4418002-Abstract.html)
- Arixiv2021,[NEUROLOGIC A Fesque Decoding: Constrained Text Generation with Lookahead Heuristics](https://arxiv.org/pdf/2112.08726.pdf)
- EMNLP2021,[Attribute Alignment: Controlling Text Generation from Pre-trained Language Models](https://arxiv.org/pdf/2103.11070.pdf)
- EACL,[Changing the Mind of Transformers for Topically-Controllable Language Generation](https://arxiv.org/pdf/2103.15335.pdf)
- Arixiv2022,[Tailor: A Prompt-Based Approach to Attribute-Based Controlled Text Generation](https://arxiv.org/pdf/2204.13362.pdf)
- Arixiv 2021,[Controllable Natural Language Generation with Contrastive Prefixes](https://arxiv.org/pdf/2202.13257.pdf)
- Arixiv2022,[COLD Decoding:Energy-based Constrained Text Generation with Langevin Dynamics](https://arxiv.org/pdf/2202.11705.pdf)
- Arixiv 2022,[AutoTemplate: A Simple Recipe for Lexically Constrained Text Generation](https://arxiv.org/pdf/2211.08387.pdf)


# Text generation
- ICLR2022,[LANGUAGE MODELING VIA STOCHASTIC PROCESSES](https://arxiv.org/pdf/2203.11370.pdf)
- ICLR2020,[Neural Text Generation with Unlikelihood Training](https://arxiv.org/abs/1908.04319)

# Data Augmentation
-EMNLP2020,[Data Boost: Text Data Augmentation Through Reinforcement Learning Guided Conditional Generation](https://arxiv.org/pdf/2012.02952.pdf)

# contrast CTG
 - ICLR2021, [CONTRASTIVE LEARNING WITH ADVERSARIAL PERTURBATIONS FOR CONDITIONAL TEXT GENERATION](https://arxiv.org/pdf/2012.07280.pdf)
 - ACL2021,  [SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization](https://arxiv.org/pdf/2106.01890.pdf)
 - ICLR2021,[FAIRFIL: CONTRASTIVE NEURAL DEBIASING METHOD FOR PRETRAINED TEXT ENCODERS](https://arxiv.org/pdf/2103.06413.pdf)

# Fine-tuning
- Arixiv,2021,[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)
- EMNLP2020,[AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980)
- arixiv 2021,[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- arixic,2021,[AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction](https://arxiv.org/abs/2104.07650)

# 文本生成（传统）
- [Syntax-Guided Controlled Generation of Paraphrases](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00318/96454/Syntax-Guided-Controlled-Generation-of-Paraphrases)

# 文本生成评测
- ACL2020,[BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/abs/2004.04696)
- Arixiv 2020, [Evaluation of Text Generation: A Survey](https://arxiv.org/abs/2006.14799)
- NIPS2021,[MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers](https://arxiv.org/pdf/2102.01454.pdf)
- AAAI2021,[Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View!](https://arxiv.org/abs/2102.08585)
- ACL2020,[BLEURT: Learning Robust Metrics for Text Generation](https://arxiv.org/pdf/2004.04696.pdf)
- AAAI2022,[InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation](https://arxiv.org/abs/2112.01589)

# date to text
- AAAI2021,[Towards Faithfulness in Open Domain Table-to-text Generation from an Entity-centric View](https://www.aaai.org/AAAI21Papers/AAAI-5176.LiuT.pdf)
- ACL2020, [Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation](https://www.aclweb.org/anthology/2020.acl-main.224.pdf)
- EACL2020,[Does the Order of Training Samples Matter? Improving Neural Data-to-Text Generation with Curriculum Learning](https://arxiv.org/pdf/2102.03554.pdf)
- EMNLP2020,[Investigating Pretrained Language Models for Graph-to-Text Generation](https://arxiv.org/pdf/2007.08426.pdf) 【graph data】
- Arixiv,[Structural Adapters in Pretrained Language Models for AMR-to-text Generation](https://arxiv.org/pdf/2103.09120.pdf)【graph data】
- Arixiv2021,[Structural Information Preserving for Graph-to-Text Generation](https://arxiv.org/pdf/2102.06749.pdf)
- EMNLP2021 findinds[Plan-then-Generate: Controlled Data-to-Text Generation via Planning](https://arxiv.org/pdf/2108.13740v1.pdf)
- EMNLP2022[Towards Table-to-Text Generation with Pretrained Language Model: A Table Structure Understanding and Text Deliberating Approach](https://arxiv.org/pdf/2301.02071.pdf)

# Formats control Text Generation

- ACL2020,[Rigid Formats Controlled Text Generation](https://arxiv.org/abs/2004.08022)



# Story telling
- AAAI2019,[Plan-And-Write: Towards Better Automatic Storytelling](https://arxiv.org/pdf/1811.05701.pdf)
- Arixiv2021,[Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes](https://arxiv.org/pdf/2104.04039.pdf)
- IJCAI2019,[Controllable Neural Story Plot Generation via Reinforcement Learning](https://arxiv.org/abs/1809.10736)  【强化学习】
- ACL2020,[MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models](https://www.aclweb.org/anthology/2020.emnlp-main.226.pdf)
- ACL2021,[COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion](https://arxiv.org/pdf/2106.02497.pdf)
- AAAI2022,[Unsupervised Editing for Counterfactual Stories](https://arxiv.org/abs/2112.05417)
- WWW2022[Genre-Controllable Story Generation via Supervised Contrastive Learning](https://dl.acm.org/doi/pdf/10.1145/3485447.3512004)
- AAAI2021[A Controllable Model of Grounded Response Generation](https://arxiv.org/abs/2005.00613)


# question rewriting
-ACL2021,[Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting](https://arxiv.org/pdf/2105.11698.pdf)

# style-tranfer
- AAAI2020,[Adapting Language Models for Non-Parallel Author-Stylized Rewriting](https://ojs.aaai.org/index.php/AAAI/article/view/6433)



# 其他
- AAAI 2020, [Fine-grained Recognition: Accounting for Subtle Differences between Similar Classes](https://arxiv.org/pdf/1912.06842.pdf),&[note](./note/2020_10.md)

- NIPS 2019, [Recurrent Space-time Graph Neural Networks](http://papers.nips.cc/paper/9444-recurrent-space-time-graph-neural-networks.pdf),&[note](./note/2020_10.md)

- ICML 2018, [Learning to Reweight Examples for Robust Deep Learning](https://arxiv.org/abs/1803.09050),&[note](./note/2020_10.md)

- ACL.2020,[A Contextual Hierarchical Attention Network with Adaptive Objective for Dialogue State Tracking](https://arxiv.org/abs/2006.01554),&[note](./note/2020_11.md)

- Arixiv 2021, [Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning](https://arxiv.org/pdf/2011.01403.pdf),&[note](./note/2020_11.md)

- ICLR 2021, [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf),&[note](./note/2020_11.md)
- ICML 2020,[Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere](https://arxiv.org/pdf/2005.10242.pdf)

- AAAI 2020, [](),&[note]()
